\documentclass[12pt,twocolumn]{IEEEtran}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{multicol}
\renewcommand{\abstractname}{Abstract}

\begin{document}
	\title{Mappa Di Coerenza Spazio-Temporale Per La Segmentazione Di Immagini Da Video}
	\author{Rea Domenico Università Degli Studi di Napoli Parthenope 0124000608}
	\date{February 2015}
	\maketitle
	\begin{abstract}
	Estrarre oggetti in movimento e salienti da video e immagini è importante per molte applicazione di sorveglianza e video-retargeting. In questo articolo useremo informazioni di coerenza spaziale e temporale per la segmentazione di oggetti da video. Mentre alcuni metodi usano le informazioni di movimento, essi non tengono conto di quelle di coerenza che potenzialmente possono portare a mappe di salienza migliori. La coerenza spaziale identifica regioni che appartengono a uno oggetto, mentre la coerenza temporale identifica oggetti in movimento con una bassa entropia, e quindi con movimento coerente. Le due mappe sono combinate per ottenere la mappa di coerenza spazio-temporale.
	L'implementazione dell'algoritmo proposto si basa sull'idea di:\newline
	D.Mahaptra (ETH Zurich)\newline
	S. Gilani (National University of Science and Technology)\newline
	M.K. Saini (University of Ottawa).
	\end{abstract}
	\section{INTRODUZIONE}
		Il sistema visivo umano è capace di distinguere gli oggetti salienti da quelli in movimento incoerente. Registrando il movimento degli occhi di alcuni osservatori muniti di eye-trackers si è riusciti a capire perchè le persone si focalizzano su alcuni oggetti di scena. Ci sono tuttora ricerche sul sviluppare modelli computazionali che imitino l'occhio umano. In alcuni metodi si usano informazioni di alto livello quali il cielo, le facce e gli umani sono usati come indicatori per identificare regioni salienti. Tuttavia informazioni del genere sono spesso scarse e non possono essere usate. Quindi molti metodi bottom-up usano informazioni di basso livello per risalire agli oggetti salienti. l'approccio generale è quello di ottenere una mappa che evidenzi gli oggetti ''interessanti'' e sopprima quelli che non lo sono. le mappe di salienza sono usate in diversi ambiti quali la videosorveglianza e la segmentazione di immagini mediche. Molte di esse non sopprimono regioni di scarso interesse o i falsi positivi.\newline
		L'introduzione del video porta con se informazioni addizionali per minimizzare i falsi positivi.
		Rispetto ai metodi per immagini statiche ci sono meno metodi per la creazione di mappe salienti e alcuni di questi hanno un costo computazionale elevato.
	\section{METODO USATO}
		La figura 1 mostra i differenti passi del metodo proposto. Una mappa di coerenza spaziale è calcolata per il frame corrente. Negli n=14 frames precedenti calcoliamo la motion entropy map, la motion center surround map, la direction entropy map e la direction center surround map. La combinazione di queste da la mappa di coerenza temporale; la quale, opportunamente combinata con la mappa di coerenza spaziale fornisce la mappa di coerenza spazio-temporale.\newline
		Alcuni approcci eseguono operazioni sia su mappe statiche chee dinamiche, il che non è una buona idea nei video. Infatti un oggetto fermo potrebbe essere saliente in termini di coerenza spaziale ma irrilevante per mancanza di movimento. La figura 2(a)(b) sono frame di video che evidenziano oggetti statici nei box rossi,quindi non elaboreremo mappe di salienza statiche ma incorporeremo le informazioni di coerenza spaziale per rilevare regioni salienti. La figura 2(c) illustra invece un oggetto saliente in movimento nella green box mentre una regione non in movimento è inclusa nella red box. Sebbene gli esempi visti mostrano che l'elaborazione di oggetti statici può essere evitata, non possiamo scartare del tutto le features di basso livello poichè contribuiscono a migliorare l'identificazione di regioni salienti.\newline
		Gli oggetti corrispondenti a regioni salienti sono segmentate usando l'algoritmo di Canny e successivamente con l'analisi delle componenti connesse. Un oggetto saliente in scene dinamiche:\newline
		1) È parte di un oggetto ben definito\newline
		2) È in foreground\newline
		3) Ha movimento coerente\newline
		4) Ha un motion profile differente da altri oggetti vicini\newline
		Un immagine di input è divisa in patch non-overlapped per ottenere sottoregioni compatte. Questo ha due vantaggi: 1) i costi computazionali si riducono siccome non dobbiamo processare ogni pixel; 2) features più accurate sono ottenute se confrontate con quelle su singoli pixel.
		La coerenza spaziale determina le parti di un oggetto. I motion vectors sono calcolati per ogni patch rispetto al frame precedente. Le informazioni di movimento (direzione e magnitudo) sono salvate per un periodo di tempo (15 frames) e analizzate per la coerenza temporale. La mappa di salienza finale identifica regioni che appartengono a un oggetto ben definito e che hanno movimento coerente.

		\begin{figure}[h]
		\caption{Illustrazione del metodo proposto}
		\begin{center}
		\includegraphics[width=9cm]{img/method}
		\end{center}
		\end{figure}
		\begin{figure}[h]
		\caption{Esempi di falsi positivi (in rosso)}
		\begin{center}
		\subfloat[]{\includegraphics[width=2.7cm]{img/fig2a}}
		\hspace*{.2cm}
		\subfloat[]{\includegraphics[width=2.7cm]{img/fig2b}}
		\hspace*{.2cm}
		\subfloat[]{\includegraphics[width=2.7cm]{img/fig2c}}		
		\end{center}
		\end{figure}
		
		\subsection{Coerenza Spaziale}
			Le mappe di coerenza spaziale identificano pixel che sono parte di un oggetto. L'istogramma dei gradienti (HoG) di una patch e la sua entropia sono calcolati a partire dall'intensità di un pixel. Sebbene le immagini a colori forniscano informazioni sul colore, ci concentreremo solo sull'Hog basato su intensità (gray-level). L'entropia dell'Hog identifica patch che fanno parte di un oggetto mentre l'analisi dei colori porta overhead senza un significativo guadagno in termini di precisione nelle mappe di salienza.\newline
			L'istogramma dei gradienti è calcolato per ciascuna patch, ciascun istogramma è diviso in 9 bins (intervalli) che rappresentano 9 possibili orientazioni del gradiente stesso. Ogni bin contiene la somma dei magnitudo di una delle 9 orientazioni. L'istogramma è poi normalizzato secondo la Norma L2 per ottenere valori nei bins compresi tra 0 e 1.
			Viene poi applicata una sogliatura per scartare valori più bassi nei bins.\newline
			Un oggetto di scena ha un equa distribuzione del gradiente sulle patch che lo compongono, ovvero ha livelli di intensità pressochè omogenei.	Se denotiamo il frame di interesse con $I_{t}$, la coerenza spaziale C per un patch s è data da:\newline
			\begin{equation}
			C(s) = -\sum_{\theta}p(\theta)\log p(\theta)
			\end{equation}
			Dove $p(\theta)$ è la probabilità del gradiente di angolo $\theta$ ed è calcolata usando la misura di distribuzione di Boltzmann:
			\begin{equation}
			p(\theta)= \frac{e^{-m(i)/k}}{\sum_{i=1}^{9}e^{-m(i)/k}}
			\end{equation}
			Dove $m(i)$ è il valore dell'i-esimo bin della patch s e k è una costante arbitraria.
		\subsection{Coerenza Temporale}
			Le mappe di coerenza temporale identificano regioni con movimento coerente nel tempo. Gli oggetti di interesse mstranoun pattern di movimento che è differente dal movimento casuale (per esempio l'ondulamento dei rami nel background dovouto al vento). L'informazione di movimento dai video può essere catturata in diversi modi quali la differenza assoluta tra frames consecutivi e l'optical flow. In questo metodo usiamo i motion vectors. I motion vectors possono essere calcolati velocemente, senza l'analisi di ciascun pixel come nell'optical flow, risparmiando tempo e risorse. Dal momento che i motion vectors sono calcolati per i blocchi, otteniamo regioni che contengono l'intero oggetto più le aree dei blocchi non coperte dall'oggetto.\newline
			I motion vectors sono usati per i block matching negli algoritmi di compressione. I blocchi (patch) del frame corrente sono confrontati con il blocco corrispondente e i blocchi vicini nel frame precedente tramite una funzione di costo. Il risultato nei motion vectors indica lo scostamento di quel blocco tra il frame precedente e il corrente. La funzione di costo utilizzata è la MAD (Mean of Absolute Difference), che ha un costo computazionale ridotto trispetto ad altre funzioni. La funzione MAD tra due blocchi i e j è definita come segue:
			\begin{equation}
			MAD(i,j) = \frac{1}{N^{2}}\sum_{n_{1}=0}^{N-1}\sum_{n_{2}=0}^{N-1} |I_{t}(n_{1},n_{2})-I_{t-1}(n_{1}+i,n_{2}+j)|
			\end{equation}
			Dove N è la dimensione del blocco; i e j sono le coordinate del primo pixel (in alto a sinistra) del blocco di riferimento. L'algoritmo per calcolare i motion vectors utilizzato è l'ARPS (Adaprive Rood Pattern Search). L'algoritmo ARPS si basa sul fatto che il movimento tra blocchi vicini è coerente, cioè se i blocchi intorno a quello di riferimento si spostano in una certa direzione allora c'è una buona probabilità che anche il blocco di riferimento si sposti in quella direzione. Nella versione base dell'ARPS i movimenti dei blocchi considerati sono quelli rilevati alla sua immediata sinistra.\newline
			Prima di iniziare a calcolare gli scostamenti il blocco viene confrontato con il medesimo blocco nel frame precedente per evitare di calcolare scostamenti nulli. 
			Nel primo passo dell'algoritmo viene settata una finestra di ricerca (ARP) per ciascun blocco in base al massimo movimento rilevato per quello alla sua sinistra, in entrambe le direzioni X e Y. Cioè:
			\begin{equation}
			ARP = max{|MV_{predicted}(x)|,|MV_{predicted}(y)|}
			\end{equation}
			Dove $MV_{predicted}(x)$ e $MV_{predicted}(y)$ sono rispettivamente i movimenti rilevati del blocco a sinistra, in pixel, in entrambi gli assi. Se il blocco in esame è quello più a sinistra si setta una finestra di ricerca fissa (Nel nostro caso ARP = 2). Di tutte le possibili posizioni della finestra ne vengono scelte 4 più un quinto punto che rappresenta proprio lo scostamento del blocco a sinistra. I 4 punti scelti sono i margini della finestra orizzontali e verticali nei quali, come mostra la figura 3, viene calcolata la MAD e salvato il punto dove la MAD ritorna il minimo valore.\newline
			Nel secondo passo dell'algoritmo viene calcolata la MAD in una seconda finestra di ricerca di dimensione arbitraria intorno al punto di minimo trovato in precedenza in cui viene applicata una full-search. Per questo motivo si sconsiglia di utilizzare una finestra superiore ai 7px.
			\begin{figure}[h]
			\caption{Scelta dei punti iniziali usati dalla MAD}
			\begin{center}
			\includegraphics[width=6cm]{img/ARP}
			\end{center}
			\end{figure}
			I motion vectors di $I_t$ hanno 2 componenti $M_x,M_y$ che indicano lo scostamento di un blocco rispetto al frame precedente nelle direzioni x e y. Calcoliamo magnitudo e $M$ e direzione $\theta_M$ come segue:\newline
			\begin{equation}
			\begin{aligned}
				M = \frac{\sqrt{M_x^2+M_y^2}}{MaxMag} \\
				\theta = arctan \frac{M_y}{M_x}
			\end{aligned}
			\end{equation}
			MaxMag è il massimo magnitudo rilevato nell'intero frame, così M è normalizzato nel range [0,1]. $\theta_M$ è calcolato usando la funzione del C++ atan2 che restituisce valori nell'intervallo [-$\pi$,$\pi$] radianti. Successivamente una traslazione dei valori negativi è effettuata per avere solo valori positivi. Infine ciascun $\theta_M$ è diviso per $2\pi$; così anche i $\theta_M$ sono normalizzati nell'intervallo [0,1].\newline
			Le mappe di coerenza temporale hanno due componenti: 1) coerenza di movimento e 2) coerenza di direzione. $M$ e $\theta_M$ sono calcolati per il frame $I_t$ rispetto al frame $I_{t-1}$ ma sono salvati per N-1 frame precedenti, dopo 15 frames calcoliamo i quattro valori utili a costruire le mappe di salienza temporale. Il primo di questi è la motion entropy definito come:
			\begin{equation}
			M_{ent}(s) = -\sum_{i=1}^{N-1}p_{M_i}\log p_{M_i}
			\end{equation}
			Dove $p_{M_i}$ è la probabilità del magnitudo i-esimo (su N-1 frames) ed è calcolata usando le informazioni di N frames e la distribuzione di Boltzmann come in precedenza.\newline
			Nei nostri esperimenti N = 15 che corrisponde a poco più di mezzo secondo se si analizzano video a 25fps (standard per molte telecamere).\newline
			La $M_{ent}$ contiene valori più alti per patch che si muovono molto. Il secondo valore rappresenta la differenza di magnitudo tra la patch $s$ e i suoi vicini. Questo concetto è chiamato center-surround map. Le regioni salienti sono quelle le cui features sono differenti da quelle che gli stanno intorno. Quindi la differenza media di magnitudo tra un blocco $s$ e i suoi vicini $N_s$ è:
			\begin{equation}
			M_{CS}(s) = \frac{1}{Z} \sum_{i=1}^{N_s}|M_s-M_i|
			\end{equation}
			dove $Z = 8$ è l'insieme 8-connesso di s. Per i bordi dell'immagine è stato usato lo Zero Padding.\newline
			Abbiamo adesso 2 mappe, $M_{ent}$ e $M_{CS}$, le quali evidenziano rispettivamente zone con elevato movimento e differente da i suoi vicini.\newline
			Un set di mappe simile si ottiene elaborando i $\theta_M$:
			\begin{equation}
			\begin{aligned}
				\theta_{ent}(s) = -\sum_{i=1}^{N-1}p_{\theta_i}\log p_{\theta_i} \\
				\theta_{CS}(s) = \frac{1}{Z} \sum_{i=1}^{N_s}|\theta_s-\theta_i|
			\end{aligned}
			\end{equation}
			Queste 2 mappe evidenziano blocchi con direzione di movimento variabile e diversa dai suoi vicini. Valori più alti di $\theta_{ent}$ indicano oggetti che hanno movimento quasi casuale, il che indica movimento incoerente. Valori alti di $\theta_{CS}$ indicano che il blocco ha una traiettoria diversa da quella dei suoi vicini, il che è un buon segno per identificare oggetti salienti.
		\subsection{Combinazione Delle Mappe}
			Le regioni salienti nella mappa finale: 1) sono parte di un oggetto; 2) hanno un movimento significativo più alto dei loro vicini; 3) Hanno una traiettoria coerente. Con le considerazioni appena fatte calcoliamo la mappa di coerenza spazio-temporale per una patch $s$:
			\begin{equation}
			SM = w(C)+(1-w)[M_{ent} \times M_{CS}+(1-\theta_{ent})\times\theta_{CS}]
			\end{equation}
			la quale fornise regioni in saliente movimento. $w = 0.3$ determina il contributo tra la coerenza spaziale e temporale. $C$ mostra valori alti per le patch che appartengono a un oggetto omogeneo. $M_{ent} \times M_{CS}$ dà una mappa che evidenzia regioni con magnitudo di movimento elevato e differente dai vicini. $(1-\theta_{ent})\times\theta_{CS}$ dà una mappa con regolare direzione di movimento e differente dai vicini. Tutte le mappe sono normalizzate nel range [0,1] prima di calcolare la mappa di salienza finale.\newline
			Per questioni di visibilità non è stata creata una mappa che abbia un punto per ogni patch ma una delle stesse dimensioni del frame in cui ciascun blocco contiene il valore della SM di quel blocco in tutti pixel del blocco stesso.
		\subsection{Segmentazione Degli Oggetti Salienti}
			valori positivi nella SM indicano regioni salienti, più alti sono i valori e più evidente è la salienza. Prima viene applicata una sogliatura a zero con un valore di soglia arbitrario (0.3 sembra essere il migliore valore), poi viene effettuata un operazione di chiusura sulla SM per coprire piccoli buchi e rendere visivamente migliore il risultato, infine l'algoritmo di Canny è applicato e un operatore traccia sul frame i contorni le cui aree che racchiudono siano superiori a una fissata soglia che, per i nostri esperimenti, dipende dalla risoluzione del video in esame nel seguente modo:
			\begin{equation}
			 Thresh = \frac{MN}{Ps^2}
			\end{equation}
			dove $M$ e $N$ sono le dimensioni dei frames e $Ps$ indica il size di una patch. Questo evita che falsi positivi siano presi in considerazione.\newline
			La figura 4 mostra l'output dei diversi stage per un frame tratto da "birdfall2", SegTrack dataset. Il frame originale viene mostrato in (a) con la segmentazione del ground truth in rosso e la nostra segmentazione in verde. La figura 3(b) mostra la SM del frame. Pixel più chiari indicano maggiore salienza. la fig. 3(c) e (d) mostrano rispettivamente  la spatial coeherency map e la temporal coeherncy map. Le figure da (e) a (h) mostrano le 4 mappe ricavate dal magnitudo $M$ e dalla direzione $\theta$.
			\pagebreak
			\begin{figure*}[ht]
			\caption{Differenti stage per un frame tratto da Birdfall2,SegTrack
			(a) frame originale,(b) mappa di coerenza spazio-temporale, (c) mappa di coerenza spaziale ottenuta con w=1, (d) mappa di coerenza temporale ottenuta con w=0, da (f) a (h) rispettivamente: Motion entropy map, Direction entropy map, Motion Center Surround map, Direction Center Surround map}
			\begin{center}
				\subfloat[]{\includegraphics[width=2.7cm]{img/1}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/2}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/7}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/8}}	
			\end{center}
			\begin{center}
				\subfloat{\includegraphics[width=2.7cm]{img/3}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/4}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/5}}
				\hspace*{.2cm}
				\subfloat[]{\includegraphics[width=2.7cm]{img/6}}		
			\end{center}
			\end{figure*}
	\section{RISULTATI E CONFRONTI}
		Il metodo è stato valutato usando 2 dataset che hanno risoluzioni differenti. le dimensioni dei frame vengono ridotte eliminando alcune righe e/o alcune colonne per far sì che diventino multipli di 8 (che è la dimensione di una patch). Tutte le immagini vengono trasformate in scala di grigi a 8 bit per l'elaborazione.
		\subsection{Risultati Visivi Della Segmentazione}
			La figura 5 mostra i risultati per i due dataset $birdfall$ e $parachute$. Ciascuna colonna rappresenta diversi frames del dataset. La segmentazione in rosso è quella ottenuta dal ground-truth mentre quella in verde è quella del metodo proposto.
		\subsection{Risultati Quantitativi}
			Sono stati utilizzati quattro indici per indicare l'efficacia del metodo proposto in confronto alla segmentazione ottimale (ground-truth) sul dataset $birdfall$. I quattro indici sono Precisione, Recupero, Dice Metric e Jaccard Index, definiti come segue:
			\begin{equation}
			\begin{aligned}
				Recall = \frac{|G \cap A|}{|G|} \\
				Precision = \frac{|G \cap A|}{|A|} \\
				DM = \frac{2|G \cap A|}{|G + A|} \\
				JI = \frac{|G \cap A|}{|G \cup A|}
			\end{aligned}
			\end{equation}
			in cui $G$ è l'insieme dei pixel del ground-truth e $A$ è l'insieme dei pixel della segmentazione. $Recall$ indica la frazione di pixel rilevanti che sono recuperati. $Precision$ indica il numero di pixel recuperati che sono rilevanti. $DM$ indica il degrado dell'overlap tra due segmentazioni. $JI$ è usato invece per confrontare le similarità tra due campioni. \newline
			L'intersezione $G \cap A$ è ottenuta binarizzando le segmentazioni e utilizzando un operatore bitwise AND tra la segmentazione ottenuta e quella del ground-truth. L'unione $G \cup A$ è invece ottenuta con un operatore bitwise OR tra la segmentazione ottenuta e quella ground-truth.\newline
			Tutti gli indici sono definiti nel range [0,1] e valori più alti indicano performance migliori. Segmentazioni ottimali (ground-truth) sono testate solo sul dataset $birdfall$ di SegTrack.\newline
			L'algoritmo implementato da Mahaptra, Gilani e Saini viene anche confrontato con altri metodi quali Optical Flow e PQFT (Phase spectrum of Quaternion Fourier Transform) ed è stato notato che solo l'algoritmo di Optical Flow è migliore del metodo proposto, sebbene sia più lento. \newline
			Gli indici di efficacia sono calcolati anche per valori di $w=0$ e $w=1$ che indicano rispettivamente una mappa ottenuta unicamente dalla Spatial Coherency map o dalla Temporal Coherency map. Notiamo che soprattutto usando solo la mappa di coerenza spaziale il risultato è inutile.\newline
			
\end{document}
